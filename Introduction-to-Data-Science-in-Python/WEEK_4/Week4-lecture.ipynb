{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.0** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-data-analysis/resources/0dhYG) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions in Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's start with a discussion of distributions. And we'll start with the most common using a coin. When a coin is flipped it has a probability of landing heads up and a probability of landing tails up. If we flip a coin many times we collect a number of measurements of the heads and tails that landed face up and intuitively we know that the number of times we get a heads up will be equal about the number of times we get a tails up for a fair coin. If you flipped a coin a hundred times and you received heads each time you'd probably doubt the fairness of that coin. We can consider the result of each flip of this coin as a random variable.**\n",
    "\n",
    "**When we can consider the set of all possible random variables together we call this a distribution. In this case the distribution is called binomial since there's two possible outputs a heads or a tails. It's also an example of a discreet distribution since there are only categories being used a heads and a tails and not real numbers.**\n",
    "\n",
    "**NumPy actually has some distributions built into it allowing us to make random flips of a coin with given parameters. Let's give it a try.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we ask for a number from the NumPy binomial distribution. We have two parameters to pass in. The first is the number of times we want it to run. The second is the chance we get a zero, which we will use to represent heads here. Let's run one round of this simulation. Great so if you're following along in a Jupyter notebook you either got a zero or a one. And half of you got a value that agreed with the one that I got.** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(1, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What if we run the simulation a thousand times and divided the result by a thousand. Well you see a number pretty close to 0.5 which means half of the time we had a heads and half of the time we had a tails. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.519"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.binomial(1000, 0.5)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Of course an even weighted binomial distribution is only one simple example. We can also have unevenly weighted binomial distributions. For instance what's the chance although we're tornado today while Iâ€™m filming. It's pretty low even though we do get tornadoes here. So maybe there a hundredth of a percentage chance. We can put this into a binomial distribution as a weighting in NumPy. If we run this 100,000 times we see there are pretty minimal tornado events. \n",
    "So you might be wondering why I'm talking about such a simple and intuitive distribution. I mean we all understand flipping a coin for when we had to make important decisions as children. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chance_of_tornado = 0.01/100\n",
    "np.random.binomial(100000, chance_of_tornado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But what I want to demonstrate is that the computational tools are starting to allow us to simulate the world which helps us answer questions. I could have shown you the math behind this so we could have worked out the probabilities. But a simulation is essentially another form of inquiry. Let's take one more example. Let's say the chance of a tornado here in Ann Arbor on any given day, is 1% regardless of the time of year. That's higher than realistic but it makes for a quicker demo. And lets say if there's a tornado I'm going to get away from the windows and hide, then come back and do my recording the next day. So what's the chance of this happening two days in a row? **\n",
    "\n",
    "**Well we can use the binomial distribution in NumPy to simulate this. **\n",
    "\n",
    "**Here we create an empty list and we create a number of potential tornado events by asking the NumPy binomial function using our chance of tornado. We'll do this a million times which is just shy of 3,000 years worth of events. **\n",
    "\n",
    "**This process is called sampling the distribution. **\n",
    "\n",
    "**Now we can write a little loop to go through the list and look for any two adjacent pairs of ones which means that there were two days that had back to back tornadoes. We see that this ends up being roughly 102 day tornado events over the 3,000 years. Which frankly is still too many for me. My point here though is that modern computational power allows us to very quickly simulate the effects of different parameters in a distribution. Leading to a new way of understanding the problem. You don't have to work out all the math you can quite often simulate the problem instead and observe the results. \n",
    "In the next lecture we'll talk a little bit more about distributions. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 tornadoes back to back in 2739.72602739726 years\n"
     ]
    }
   ],
   "source": [
    "chance_of_tornado = 0.01\n",
    "\n",
    "tornado_events = np.random.binomial(1, chance_of_tornado, 1000000)\n",
    "    \n",
    "two_days_in_a_row = 0\n",
    "for j in range(1,len(tornado_events)-1):\n",
    "    if tornado_events[j]==1 and tornado_events[j-1]==1:\n",
    "        two_days_in_a_row+=1\n",
    "\n",
    "print('{} tornadoes back to back in {} years'.format(two_days_in_a_row, 1000000/365))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Many of the distributions you use in data science are not discrete binomial, and instead are continues where the value of the given observation isn't a category like heads or tails, but can be represented by a real number. It's common to then graph these distributions when talking about them, where the x axis is the value of the observation and the y axis represents the probability that a given observation will occur. **\n",
    "\n",
    "**If all numbers are equally likely to be drawn when you sample from it, this should be graphed as a flat horizontal line. And this flat line is actually called the uniform distribution. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5425320297074238"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are few other distributions that get a lot more interesting. Let's take the normal distribution which is also called Gaussian Distribution or sometimes, a Bell Curve. **\n",
    "\n",
    "**This distribution looks like a hump where the number which has the highest probability of being drawn is a zero, and there are two decreasing curves on either side of the X axis.** \n",
    "\n",
    "**One of the properties of this distribution is that the mean is zero, not the two curves on either side are symmetric. I want to introduce you to the term expected value. I think that most of us are familiar with the mean is the sum of all the values divided by the total number of values. Calculating a mean values are computational process, and it takes place by looking at samples from distribution. For instance rolling a die three times might give you 1, 2 and 6, the mean value is then 4.5. The expected value is the probability from the underlying distribution is what would be the mean of a die roll if we did an infinite number of rolls. The result is 3.5 since each face of the die is equally likely to appear. Thus the expected value is 3.5, while the mean value depends upon the samples that we've taken, and converges to the expected value given a sufficiently large sample set. We'll talk more about expected values in course three. A second property is that the variance of the distribution can be described in a certain way. Variance is a measure of how badly values of samples are spread out from the mean. Let's get a little bit more formal about five different characteristics of distributions. First, we can talk about the distribution central tendency, and the measures we would use for this are mode, median, or mean. This characteristic is really about where the bulk of probability is in the distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.061217752906879"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can also talk about the variability in the distribution. There are a couple of ways we can speak of this. The standard deviation is one, the interquartile range is another. The standard deviation is simply a measure of how different each item, in our sample, is from the mean.**\n",
    "\n",
    "**Here's the formula for standard deviation.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formula for standard deviation:\n",
    "$$\\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\overline{x})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It might look a little more intimidating than it actually is. Let's just walk through how we would write this up. Let's draw 1,000 samples from a normal distribution with an expected value of 0.75 and a standard deviation of 1. Then we calculate the actual mean using NumPy's mean feature. The part inside the summation says xi- x bar. Xi is the current item in the list and x bar is the mean. So we calculate the difference, then we square the result, then we sum all of these.**\n",
    "\n",
    "** This might be a reasonable place to use a map and apply a lambda to calculate the differences between the mean and the measured value. Then to convert this back to a list, so NumPy can use it. Now we just have to square each value, sum them together, and take the square root. So that's the size of our standard deviation. It covers roughly 68% of the area around the mean, split evenly around the side of the mean. Now we don't normally have to do all this work ourselves, but I wanted to show you how you can sample from the distribution, create a precise programmatic description of a formula, and apply it to your data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0692809846960383"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distribution = np.random.normal(0.75,size=1000)\n",
    "\n",
    "np.sqrt(np.sum((np.mean(distribution)-distribution)**2)/len(distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** But for standard deviation, which is just one particular measure of variability, NumPy has a built-in function that you can apply, called STD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0692809846960383"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** There's a couple more measures of distribution that are interesting to talk about. One of these is the shape of the tales of the distribution and this is called the kurtosis. We can measure the kurtosis using the statistics functions in the SciPy package. A negative value means the curve is slightly more flat than a normal distribution, and a positive value means the curve is slightly more peaky than a normal distribution. Remember that we aren't measuring the kurtosis of the distribution per se, but of the thousand values which we sampled out of the distribution. This is a sublet but important distinction.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08083348158692827"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "stats.kurtosis(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We could also move out of the normal distributions and push the peak of the curve one way or the other. And this is called the skew. \n",
    "If we test our current sample data, we see that there isn't much of a skew**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.07419952935266544"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.skew(distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Let's switch distributions and take a look at a distribution called the Chi Squared distribution, which is also quite commonly used in statistic. The Chi Squared Distribution has only one parameter called the degrees of freedom. The degrees of freedom is closely related to the number of samples that you take from a normal population. It's important for significance testing. But what I would like you to observe, is that as the degrees of freedom increases, the shape of the Chi Squared distribution changes. In particular, the skew to the left begins to move towards the center. We can observe this through simulation.**\n",
    "\n",
    "** First we'll sample 1,000 values from a Chi Squared distribution with degrees of freedom 2. Now we can see that the skew is quite large.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0401293451908944"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared_df2 = np.random.chisquare(2, size=10000)\n",
    "stats.skew(chi_squared_df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now if we re-sample changing degrees of freedom to 5. \n",
    "We see that the skew has decreased significantly.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2289905482870283"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared_df5 = np.random.chisquare(5, size=10000)\n",
    "stats.skew(chi_squared_df5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9459928328570312"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared_df10 = np.random.chisquare(10, size=10000)\n",
    "stats.skew(chi_squared_df10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4133552638960732"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_squared_df50 = np.random.chisquare(50, size=10000)\n",
    "stats.skew(chi_squared_df50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can actually plot this right in the Jupiter notebook. I'm not going to talk much about the library we're using here for plotting, because that's the topic of the next course.**\n",
    "\n",
    "**But you can see a histogram with our plot with the two degrees of freedom is skewed much further to the left, while our plot with the five degrees of freedom is not as highly skewed.** \n",
    "\n",
    "**I could encourage you as always to play with this notebook and change the parameters and see how the degrees of freedom changes the skew of the distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1a20cbf6a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "output = plt.hist([chi_squared_df2,chi_squared_df5,chi_squared_df10, chi_squared_df50], bins=50, histtype='step', \n",
    "                  label=['2 degrees of freedom','5 degrees of freedom', '10 degrees of freedom', '50 degrees of freedom'])\n",
    "plt.legend(loc='upper right')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The last aspect of distributions that I want to talk about is the modality. So far, all of the distributions I've shown have a single high point, a peak. But what if we have multiple peaks? This distribution has two high points, so we call it bimodal. These are really interesting distributions and happen regularly in data mining. We're going to talk a bit more about them in course three. But a useful insight is that we can actually model these using two normal distributions with different parameters. These are called Gaussian Mixture Models and are particularly useful when clustering data.**\n",
    "\n",
    "**Well, this has been a long lecture but I think it can be tough to chop up a discussion of distributions and still get the main points across. Remember that a distribution is just a shape that describes the probability of a value being pulled when we sample a population. And NumPy and SciPy each have a number of different distributions built in for us to be able to sample from.**\n",
    "\n",
    "**The last point I want to leave you with here is a reference. If you find this way of exploring statistics interesting. Alan Downey wrote a nice book called Think Stats by the publisher O'Reilly. I think he does a really nice job of teaching how to think about statistics from a programming perspective. One where you write the functions behind the statistical methods. It's not really a reference book, but it's an interesting way to approach learning the fundamentals of statistics.**\n",
    "\n",
    "**Allen even has a free copy of this book available on his website in PDF format and, of course, all of the code is done in Python.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We use statistics in a lot of different ways in data science. In this lecture, I want to refresh your knowledge of hypothesis testing, which is a core data analysis activity behind experimentation. We're starting to see experimentation used more and more commonly outside of academic scientific, and in day to day business environments. Part of the reason for this is the rise of big data and web commerce. It's now easy to change your digital storefront and deliver a different experience to some of your customers, and then see how those customer actions might differ from one another. For instance, if you sell books, you might want to have one condition where the cover of the book is featured on the web page and another condition where the focus is on the author and the reviews of the book. This is often called A/B testing. And while it's not unique to this time in history, it's now becoming so common that if you're using a website, you are undoubtedly part of an A/B test somewhere. This raises some interesting ethical questions and I've added a reading to the course resources and I would like to encourage you to check it out and join in the discussion, but let's refocus back on statistics.**\n",
    "\n",
    "**A hypothesis is a statement that we can test. I'll pull an example from my own research of educational technology and learning analytics. Let's say that we have an expectation that when a new course is launched on a MOOC platform, the keenest students find out about it and all flock to it. Thus, we might expect that those students who sign up quite quickly after the course is launched with higher performance than those students who signed up after the MOOC has been around for a while. In this example, we have samples from two different groups which we want to compare. The early sign ups and the late sign ups. **\n",
    "\n",
    "**When we do hypothesis testing, we hold out that our hypothesis as the alternative and we create a second hypothesis called the null hypothesis, which in this case would be that there is no difference between groups. We then examine the groups to determine whether this null hypothesis is true or not.**\n",
    "\n",
    "**If we find that there is a difference between groups, then we can reject the null hypothesis and we accept our alternative. There are subtleties in this description. We aren't saying that our hypothesis is true per se, but we're saying that there's evidence against the null hypothesis. So, we're more confident in our alternative hypothesis. Let's see an example of this. We can load a file called grids.csv.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('grades.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>assignment1_grade</th>\n",
       "      <th>assignment1_submission</th>\n",
       "      <th>assignment2_grade</th>\n",
       "      <th>assignment2_submission</th>\n",
       "      <th>assignment3_grade</th>\n",
       "      <th>assignment3_submission</th>\n",
       "      <th>assignment4_grade</th>\n",
       "      <th>assignment4_submission</th>\n",
       "      <th>assignment5_grade</th>\n",
       "      <th>assignment5_submission</th>\n",
       "      <th>assignment6_grade</th>\n",
       "      <th>assignment6_submission</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B73F2C11-70F0-E37D-8B10-1D20AFED50B1</td>\n",
       "      <td>92.733946</td>\n",
       "      <td>2015-11-02 06:55:34.282000000</td>\n",
       "      <td>83.030552</td>\n",
       "      <td>2015-11-09 02:22:58.938000000</td>\n",
       "      <td>67.164441</td>\n",
       "      <td>2015-11-12 08:58:33.998000000</td>\n",
       "      <td>53.011553</td>\n",
       "      <td>2015-11-16 01:21:24.663000000</td>\n",
       "      <td>47.710398</td>\n",
       "      <td>2015-11-20 13:24:59.692000000</td>\n",
       "      <td>38.168318</td>\n",
       "      <td>2015-11-22 18:31:15.934000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98A0FAE0-A19A-13D2-4BB5-CFBFD94031D1</td>\n",
       "      <td>86.790821</td>\n",
       "      <td>2015-11-29 14:57:44.429000000</td>\n",
       "      <td>86.290821</td>\n",
       "      <td>2015-12-06 17:41:18.449000000</td>\n",
       "      <td>69.772657</td>\n",
       "      <td>2015-12-10 08:54:55.904000000</td>\n",
       "      <td>55.098125</td>\n",
       "      <td>2015-12-13 17:32:30.941000000</td>\n",
       "      <td>49.588313</td>\n",
       "      <td>2015-12-19 23:26:39.285000000</td>\n",
       "      <td>44.629482</td>\n",
       "      <td>2015-12-21 17:07:24.275000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D0F62040-CEB0-904C-F563-2F8620916C4E</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 05:36:02.389000000</td>\n",
       "      <td>85.512541</td>\n",
       "      <td>2016-01-09 06:39:44.416000000</td>\n",
       "      <td>68.410033</td>\n",
       "      <td>2016-01-15 20:22:45.882000000</td>\n",
       "      <td>54.728026</td>\n",
       "      <td>2016-01-11 12:41:50.749000000</td>\n",
       "      <td>49.255224</td>\n",
       "      <td>2016-01-11 17:31:12.489000000</td>\n",
       "      <td>44.329701</td>\n",
       "      <td>2016-01-17 16:24:42.765000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FFDF2B2C-F514-EF7F-6538-A6A53518E9DC</td>\n",
       "      <td>86.030665</td>\n",
       "      <td>2016-04-30 06:50:39.801000000</td>\n",
       "      <td>68.824532</td>\n",
       "      <td>2016-04-30 17:20:38.727000000</td>\n",
       "      <td>61.942079</td>\n",
       "      <td>2016-05-12 07:47:16.326000000</td>\n",
       "      <td>49.553663</td>\n",
       "      <td>2016-05-07 16:09:20.485000000</td>\n",
       "      <td>49.553663</td>\n",
       "      <td>2016-05-24 12:51:18.016000000</td>\n",
       "      <td>44.598297</td>\n",
       "      <td>2016-05-26 08:09:12.058000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5ECBEEB6-F1CE-80AE-3164-E45E99473FB4</td>\n",
       "      <td>64.813800</td>\n",
       "      <td>2015-12-13 17:06:10.750000000</td>\n",
       "      <td>51.491040</td>\n",
       "      <td>2015-12-14 12:25:12.056000000</td>\n",
       "      <td>41.932832</td>\n",
       "      <td>2015-12-29 14:25:22.594000000</td>\n",
       "      <td>36.929549</td>\n",
       "      <td>2015-12-28 01:29:55.901000000</td>\n",
       "      <td>33.236594</td>\n",
       "      <td>2015-12-29 14:46:06.628000000</td>\n",
       "      <td>33.236594</td>\n",
       "      <td>2016-01-05 01:06:59.546000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             student_id  assignment1_grade  \\\n",
       "0  B73F2C11-70F0-E37D-8B10-1D20AFED50B1          92.733946   \n",
       "1  98A0FAE0-A19A-13D2-4BB5-CFBFD94031D1          86.790821   \n",
       "2  D0F62040-CEB0-904C-F563-2F8620916C4E          85.512541   \n",
       "3  FFDF2B2C-F514-EF7F-6538-A6A53518E9DC          86.030665   \n",
       "4  5ECBEEB6-F1CE-80AE-3164-E45E99473FB4          64.813800   \n",
       "\n",
       "          assignment1_submission  assignment2_grade  \\\n",
       "0  2015-11-02 06:55:34.282000000          83.030552   \n",
       "1  2015-11-29 14:57:44.429000000          86.290821   \n",
       "2  2016-01-09 05:36:02.389000000          85.512541   \n",
       "3  2016-04-30 06:50:39.801000000          68.824532   \n",
       "4  2015-12-13 17:06:10.750000000          51.491040   \n",
       "\n",
       "          assignment2_submission  assignment3_grade  \\\n",
       "0  2015-11-09 02:22:58.938000000          67.164441   \n",
       "1  2015-12-06 17:41:18.449000000          69.772657   \n",
       "2  2016-01-09 06:39:44.416000000          68.410033   \n",
       "3  2016-04-30 17:20:38.727000000          61.942079   \n",
       "4  2015-12-14 12:25:12.056000000          41.932832   \n",
       "\n",
       "          assignment3_submission  assignment4_grade  \\\n",
       "0  2015-11-12 08:58:33.998000000          53.011553   \n",
       "1  2015-12-10 08:54:55.904000000          55.098125   \n",
       "2  2016-01-15 20:22:45.882000000          54.728026   \n",
       "3  2016-05-12 07:47:16.326000000          49.553663   \n",
       "4  2015-12-29 14:25:22.594000000          36.929549   \n",
       "\n",
       "          assignment4_submission  assignment5_grade  \\\n",
       "0  2015-11-16 01:21:24.663000000          47.710398   \n",
       "1  2015-12-13 17:32:30.941000000          49.588313   \n",
       "2  2016-01-11 12:41:50.749000000          49.255224   \n",
       "3  2016-05-07 16:09:20.485000000          49.553663   \n",
       "4  2015-12-28 01:29:55.901000000          33.236594   \n",
       "\n",
       "          assignment5_submission  assignment6_grade  \\\n",
       "0  2015-11-20 13:24:59.692000000          38.168318   \n",
       "1  2015-12-19 23:26:39.285000000          44.629482   \n",
       "2  2016-01-11 17:31:12.489000000          44.329701   \n",
       "3  2016-05-24 12:51:18.016000000          44.598297   \n",
       "4  2015-12-29 14:46:06.628000000          33.236594   \n",
       "\n",
       "          assignment6_submission  \n",
       "0  2015-11-22 18:31:15.934000000  \n",
       "1  2015-12-21 17:07:24.275000000  \n",
       "2  2016-01-17 16:24:42.765000000  \n",
       "3  2016-05-26 08:09:12.058000000  \n",
       "4  2016-01-05 01:06:59.546000000  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** If we take a look at the data frame inside, we see we have six different assignments. Each with a submission time and it looks like there just under 3,000 entries in this data file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2315"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For the purpose of this lecture, let's segment this population in to two pieces. Those who finish the first assignment by the end of December 2015 and those who finish it sometimes after that. \n",
    "3:05\n",
    "I just made this date up and it gives us two data frames, which are roughly the same size. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "early = df[df['assignment1_submission'] <= '2015-12-31']\n",
    "late = df[df['assignment1_submission'] > '2015-12-31']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you've seen, the pandas data frame object has a variety of statistical functions associated with it. If we call the mean function directly on the data frame, we see that each of the means for the assignments are calculated. \n",
    "Note that the date time values are ignored as panda's knows this isn't a number, but an object type.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assignment1_grade    74.972741\n",
       "assignment2_grade    67.252190\n",
       "assignment3_grade    61.129050\n",
       "assignment4_grade    54.157620\n",
       "assignment5_grade    48.634643\n",
       "assignment6_grade    43.838980\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If we look at the mean values for the late data frame as well, we get surprisingly similar numbers. There are slight differences, though. It looks like the end of the six assignments, the early users are doing better by about a percentage point. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "assignment1_grade    74.017429\n",
       "assignment2_grade    66.370822\n",
       "assignment3_grade    60.023244\n",
       "assignment4_grade    54.058138\n",
       "assignment5_grade    48.599402\n",
       "assignment6_grade    43.844384\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**So, is this enough to go ahead and make some interventions to actually try and change something in the way we teach? When doing hypothesis testing, we have to choose a significance level as a threshold for how much of a chance we're willing to accept. This significance level is typically called alpha. It can vary greatly, depending on what you're going to do with the result and the amount of noise you expect in your data.**\n",
    "\n",
    "**For instance, in social sciences research, a value of 0.05 or 0.01 is often used, which indicates a tolerance for a probability of between 5% and 1% of chance. In a physics experiment where the conditions are much more controlled and thus, the burden of proof is much higher, you might expect to see alpha levels of 10 to the negative 5 or 100,000th of a percentage.**\n",
    "\n",
    "**You can think of the significance level from the perspective of interventions as well and this is something I run into regularly with my research. What am I going to do when I find out that two student populations are different? For instance, if I'm going to send an email nudge to encourage students to continue working on their homework, that's a pretty low-cost intervention. Emails are cheap and while I certainly don't want to annoy students, one extra email isn't going to ruin their day. But what if the intervention is a little more involved, like having our tutorial assistant followup with a student via phone? This is all of a sudden much more expensive for both the institution and for the student. So, I might want to ensure a higher burden of proof.** \n",
    "\n",
    "**So the threshold you set for alpha depends on what you might do with the result, as well. For this example, let's use a threshold of 0.05 for our alpha or 5%. Now, how do we actually test whether these means are different in Python? The SciPy library contains a number of different statistical tests and forms a basis for hypothesis testing in Python. A T test is one way to compare the means of two different populations. In the SciPy library, the T test end function will compare two independent samples to see if they have different means. I'm not going to go into the details of any of this statistical test here. But instead, we'd recommend that you check out the Wikipedia page on particular test or consider taking a full statistics course if this is unfamiliar to you. But I do want to note that most statistical tests expect that the data conforms to a certain distribution, a shape. So, you shouldn't apply such tests blindly and should investigate your data first.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# stats.ttest_ind?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** If we want to compare the assignment grades for the first assignment between the two populations, we could generate a T test by passing these two series into the T test in function. The result is a two with a test statistic and a p-value. The p-value here is much larger than our 0.05. So we cannot reject the null hypothesis, which is that the two populations are the same. In more late terms, we would say that there's no statistically significant difference between these two sample means.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.400549944897566, pvalue=0.16148283016060577)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(early['assignment1_grade'], late['assignment1_grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's check with assignment two grade. No, that's much larger than 0.052**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.3239868220912567, pvalue=0.18563824610067967)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(early['assignment2_grade'], late['assignment2_grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** How about with assignment three? Well, that's much closer, but still beyond our threshold value.\n",
    "It's important to stop here and talk about serious process from with how we're handling this investigation of the difference between these two populations. When we set the alpha to be 0.05, we're saying that we expect it that there will be positive result, 5% of the time just to the chance. **\n",
    "\n",
    "**As we run more and more T tests, we're more likely to find a positive result just because of the number of T tests we have run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.7116160037010733, pvalue=0.08710151634155668)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(early['assignment3_grade'], late['assignment3_grade'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "** When a data scientist run many tests in this way, it's called p-hacking or dredging and it's a serious methodological issue. P-hacking results in spurious correlations instead of generalizable results. There are a couple of different ways you can deal with p-hacking. The first is called the Bonferroni correction. In this case, you simply tighten your alpha value, the threshold of significance based on the number of tests you're running. So if you choose 0.05 with 1 test, you want to run 3 test, you reduce alpha by multiplying 0.05 by one-third to get a new value of 0.01 sub. I personally find this approach to be very conservative. Another option is to hold out some of your data for testing to see how generizable your result is. In this case, we might take half of our data for each of the two data frames, run our T test with that. Form specific hypothesis based on the result of these tests, then run very limited tests on the rest of the data.**\n",
    "\n",
    "**This method is actually heavily used in machine learning when building predictive models where it's called cross fold validation and you'll learn more about this in third course in this specialization.**\n",
    "\n",
    "**A final method which has come about is the pre-registration of your experiment.** \n",
    "\n",
    "**In this step, you would outline what you expect to find and why, and describe the test that would backup a positive proof of this. You register it with a third party, in academic circles, this is often a journal who determines whether it's a reasonable test or rather not.**\n",
    "\n",
    "**You then run your study and report the results, regardless as to whether they were positive or not. Here, there is a larger burden on connecting to existing theory since you need to convince reviewers that the experiment is likely to test fully a given hypothesis. In this lecture, we've discussed just some of the basics of hypothesis testing in Python. I introduced you to the SciPy library, which you can use for T testing. We've discussed some of the practical issues which arise from looking for statistical significance. There's much more to learn about hypothesis testing. For instance, there are different tests used, depending on the shape of your data and different ways to report results instead of just p-values such as confidence intervals. But I hope this gives you a start to comparing the means of two different populations, which is a common task for data scientists and we'll followup some of this work in the second course in this series.**\n",
    "\n",
    "**This lecture also completes the lectures for the first course in the Applied Data Science with Python Specialization. We've covered the basics of Python programming, some more advanced features like maps, lambdas and miscomprehensions. How to read and manipulate data using the panda's library, including querying, joining, grouping and processing of data frames and the creation of pivot tables. And now we've talked a little bit about statistics in Python and dug deeper into the Num-Py and SciPy toolkit. In the next course, we'll dig into plotting and charting of data. Dealing a bit more with statistics and how we present data to others, and how we build a compelling story for the data we have. I'll see you then.**\n",
    "\n",
    "___\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
